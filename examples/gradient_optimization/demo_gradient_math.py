"""
DEMONSTRATE GRADIENT COMPUTATION - Real Mathematics
===================================================

This script shows the ACTUAL gradient computation that happens
during optimization with real numbers.
"""

import numpy as np
import sys
sys.path.append(r"f:\MDO LAB\_RootFiles")

print("=" * 80)
print("GRADIENT COMPUTATION DEMONSTRATION")
print("=" * 80)

# Simplified example: 5 Y-coordinates instead of 150
print("\nüìê Simplified Airfoil (5 points for clarity):")
print("-" * 80)

x_coords = np.array([0.0, 0.25, 0.5, 0.75, 1.0])
y_coords = np.array([0.0, 0.03, 0.04, 0.02, 0.0])  # Leading/trailing edge at 0

print(f"X coordinates: {x_coords}")
print(f"Y coordinates: {y_coords}")

# Mock NeuralFoil function (simplified physics)
def mock_neuralfoil(coords):
    """Simplified aerodynamics: CL and CD from geometry"""
    y = coords[:, 1]
    
    # Lift: proportional to camber (area under curve)
    camber = np.trapz(y, coords[:, 0])
    CL = 2 * np.pi * camber * 10  # Thin airfoil theory approximation
    
    # Drag: friction + pressure drag (depends on thickness)
    thickness = np.max(y) - np.min(y)
    CD_friction = 0.004
    CD_pressure = 0.001 + (thickness - 0.04)**2 * 10
    CD = CD_friction + CD_pressure
    
    LD = CL / CD if CD > 0 else 0
    
    return {'CL': CL, 'CD': CD, 'LD': LD}

# Build coordinate array
coords_baseline = np.column_stack([x_coords, y_coords])

print(f"\nüìä Baseline Performance:")
print("-" * 80)
perf_baseline = mock_neuralfoil(coords_baseline)
print(f"CL = {perf_baseline['CL']:.4f}")
print(f"CD = {perf_baseline['CD']:.6f}")
print(f"L/D = {perf_baseline['LD']:.2f}")

print(f"\nüî¨ Computing Gradient via Finite Differences:")
print("-" * 80)
print(f"Method: ‚àÇ(L/D)/‚àÇy·µ¢ ‚âà [L/D(y·µ¢ + Œµ) - L/D(y·µ¢)] / Œµ")
print(f"Step size: Œµ = 1√ó10‚Åª‚Å∏")

epsilon = 1e-8
gradient = np.zeros(len(y_coords))

print(f"\nPoint | Y_baseline | Y_perturbed | L/D_base | L/D_pert | Gradient")
print("-" * 80)

for i in range(len(y_coords)):
    # Skip fixed points (leading/trailing edge)
    if i == 0 or i == len(y_coords) - 1:
        gradient[i] = 0.0
        print(f"  {i}   | {y_coords[i]:.6f} |    FIXED    | {perf_baseline['LD']:8.2f} |    --    |   FIXED")
        continue
    
    # Perturb coordinate i
    y_perturbed = y_coords.copy()
    y_perturbed[i] += epsilon
    
    coords_perturbed = np.column_stack([x_coords, y_perturbed])
    perf_perturbed = mock_neuralfoil(coords_perturbed)
    
    # Compute gradient component
    gradient[i] = (perf_perturbed['LD'] - perf_baseline['LD']) / epsilon
    
    print(f"  {i}   | {y_coords[i]:.6f} | {y_perturbed[i]:.6f} | "
          f"{perf_baseline['LD']:8.2f} | {perf_perturbed['LD']:8.2f} | "
          f"{gradient[i]:+9.1f}")

print(f"\n‚úÖ Gradient Vector:")
print("-" * 80)
print(f"‚àá(L/D) = {gradient}")
print(f"\nGradient Magnitude: ||‚àá(L/D)|| = {np.linalg.norm(gradient):.1f}")

print(f"\nüéØ Physical Interpretation:")
print("-" * 80)
for i in range(1, len(y_coords)-1):
    if gradient[i] > 0:
        direction = "UP ‚¨ÜÔ∏è  (increase camber)"
        effect = "INCREASES L/D"
    else:
        direction = "DOWN ‚¨áÔ∏è  (decrease camber)"
        effect = "INCREASES L/D"
    
    print(f"Point {i} at x={x_coords[i]:.2f}: Gradient = {gradient[i]:+8.1f}")
    print(f"  ‚Üí Moving {direction}")
    print(f"  ‚Üí Effect: {effect}")
    print()

print(f"\n‚öôÔ∏è  Computing Search Direction (Simplified Newton Step):")
print("-" * 80)
print(f"For this demo, using steepest ascent: d = +‚àá(L/D)")
print(f"(Real L-BFGS-B uses: d = -H‚Åª¬π¬∑‚àáf where H is Hessian approximation)")

search_direction = gradient.copy()
search_direction[0] = 0  # Don't move LE
search_direction[-1] = 0  # Don't move TE

print(f"\nSearch Direction: d = {search_direction}")

print(f"\nüöÄ Taking Optimization Step:")
print("-" * 80)

step_size = 0.001  # Alpha from line search
y_new = y_coords + step_size * search_direction

print(f"Step size: Œ± = {step_size}")
print(f"\n   Point | Y_old      | Direction  | Step      | Y_new")
print("-" * 80)
for i in range(len(y_coords)):
    step = step_size * search_direction[i]
    print(f"     {i}   | {y_coords[i]:.6f}  | {search_direction[i]:+9.1f} | "
          f"{step:+.6f} | {y_new[i]:.6f}")

# Evaluate new coordinates
coords_new = np.column_stack([x_coords, y_new])
perf_new = mock_neuralfoil(coords_new)

print(f"\nüìä Performance After Step:")
print("-" * 80)
print(f"   Metric    | Baseline  | After Step | Change")
print("-" * 80)
print(f"   L/D       | {perf_baseline['LD']:9.2f} | {perf_new['LD']:10.2f} | "
      f"{perf_new['LD'] - perf_baseline['LD']:+7.2f}")
print(f"   CL        | {perf_baseline['CL']:9.4f} | {perf_new['CL']:10.4f} | "
      f"{perf_new['CL'] - perf_baseline['CL']:+7.4f}")
print(f"   CD        | {perf_baseline['CD']:9.6f} | {perf_new['CD']:10.6f} | "
      f"{perf_new['CD'] - perf_baseline['CD']:+7.6f}")

improvement = ((perf_new['LD'] - perf_baseline['LD']) / perf_baseline['LD']) * 100
print(f"\n‚úÖ L/D Improvement: {improvement:+.2f}%")

print(f"\n" + "=" * 80)
print("KEY INSIGHTS")
print("=" * 80)
print("""
1. GRADIENT COMPUTATION:
   ‚Ä¢ Each point needs 1 perturbed evaluation
   ‚Ä¢ For 150 points ‚Üí 150 NeuralFoil calls
   ‚Ä¢ Tells us which direction improves L/D

2. PHYSICAL MEANING:
   ‚Ä¢ Positive gradient ‚Üí moving UP increases L/D
   ‚Ä¢ Negative gradient ‚Üí moving DOWN increases L/D
   ‚Ä¢ Zero gradient ‚Üí at local optimum (or fixed point)

3. OPTIMIZATION STEP:
   ‚Ä¢ Move in direction of gradient
   ‚Ä¢ Step size from line search (balance speed vs accuracy)
   ‚Ä¢ Repeat until gradient ‚Üí 0 (convergence)

4. REAL IMPLEMENTATION:
   ‚Ä¢ 150 coordinates (not 5)
   ‚Ä¢ L-BFGS-B uses Hessian approximation (not just gradient)
   ‚Ä¢ Respects bounds (LE/TE fixed, Y ¬±5%)
   ‚Ä¢ Takes ~50 iterations to converge
""")

print("=" * 80)
print("\nüéì THIS IS THE ACTUAL MATHEMATICS BEHIND THE OPTIMIZER!")
print("=" * 80)
